{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY2ef3IwSUbU"
      },
      "source": [
        "# Baseline v3 - Unsloth 최적화 버전\n",
        "\n",
        "이 베이스라인 코드는 기존 v3 코드를 **Unsloth 라이브러리**를 사용하여 최적화한 버전입니다.\n",
        "\n",
        "**주요 변경 사항:**\n",
        "- **Unsloth 적용**: 더 적은 메모리로 2배 이상 빠른 파인튜닝을 수행합니다.\n",
        "- **SFTTrainer 사용**: Transformers의 복잡한 수동 학습 루프를 `trl`의 `SFTTrainer`로 대체하여 코드를 간소화했습니다.\n",
        "- **`FastVisionModel` 사용**: Unsloth의 Vision-Language 모델 로딩 클래스를 사용하여 모델 로딩 및 양자화 과정을 단순화했습니다.\n",
        "\n",
        "Colab의 GPU 환경(T4 GPU)에서 개발되었습니다.\n",
        "- **런타임 > 런타임 유형 변경 > T4 GPU**로 설정해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HBjYvBZSzqr"
      },
      "source": [
        "# 1. 환경 준비\n",
        "\n",
        "Unsloth 및 최신 라이브러리를 설치합니다.\n",
        "\n",
        "- 아래 셀 실행 후 **런타임 > 세션 다시 시작**을 반드시 진행해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unsloth_install"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import torch\n",
        "if \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    print(\"Installing Unsloth for Google Colab...\")\n",
        "    v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo -q\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer -q\n",
        "    !pip install --no-deps unsloth -q\n",
        "#    !pip install --no-deps --upgrade timm -q # For Gemma 3N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0wuwJLPTJpQ"
      },
      "source": [
        "# 2. 데이터 준비\n",
        "\n",
        "구글 드라이브를 마운트하고 대회 데이터를 압축 해제합니다.   \n",
        "(*kaggle로 진행하는 교육생들은 `/kaggle/input/`에 있는 데이터셋으로 진행해주세요!)\n",
        "\n",
        "- `train.csv`, `train/`\n",
        "- `test.csv`, `test/`\n",
        "- `sample_submission.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXxjjvUpTLMA"
      },
      "outputs": [],
      "source": [
        "# 구글드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW4YNdl0Tn70"
      },
      "outputs": [],
      "source": [
        "# 데이터 압축 해제 (약 1분 소요)\n",
        "!unzip -q \"/content/drive/My Drive/251023/data.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WO2OCFiTqDv"
      },
      "source": [
        "# 3. 라이브러리, 데이터, 설정 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QXMeMgFT5MT"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from unsloth import FastVisionModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 이미지 로드 시 픽셀 제한 해제\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "# 디바이스 GPU 우선 사용 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 설정값 정의\n",
        "MODEL_ID = \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\" # Unsloth의 4bit 양자화 모델\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "MAX_NEW_TOKENS = 8\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 데이터셋 로드\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df  = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# 학습 시간을 줄이기 위해 200개 샘플만 사용 (실제 대회에서는 전체 데이터를 사용하세요)\n",
        "train_df = train_df.sample(n=200, random_state=SEED).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLdJb59EULQv"
      },
      "source": [
        "# 4. Unsloth 모델 및 LoRA 어댑터 로딩\n",
        "\n",
        "Unsloth의 `FastVisionModel`을 사용하여 4bit 양자화된 모델과 토크나이저를 간편하게 로드합니다. 이후 LoRA 어댑터를 추가하여 학습 준비를 마칩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbinSBK_Ubgo"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")\n",
        "\n",
        "# LoRA 어댑터 추가\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = SEED,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvJAhuBvUnRe"
      },
      "source": [
        "# 5. 프롬프트 템플릿 및 데이터 포맷팅\n",
        "\n",
        "학습 및 추론에 사용할 프롬프트 템플릿을 정의하고, Hugging Face `datasets` 라이브러리를 사용하여 데이터를 모델의 Chat Template에 맞게 변환합니다. 이 과정은 기존의 Custom Dataset/Dataloader를 대체합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_formatting"
      },
      "outputs": [],
      "source": [
        "# 시스템 프롬포트도 커스터마이징 필수! (정확률 5%~15% 향상)\n",
        "SYSTEM_INSTRUCT = (\n",
        "    \"You are a helpful visual question answering assistant. \"\n",
        "    \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
        ")\n",
        "\n",
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    return (\n",
        "        f\"{question}\\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        \"정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.\"\n",
        "    )\n",
        "\n",
        "# 학습 데이터를 모델의 대화 형식(messages)으로 변환하는 함수\n",
        "def format_train_data(example):\n",
        "    img = Image.open(example[\"path\"]).convert(\"RGB\")\n",
        "    user_text = build_mc_prompt(str(example[\"question\"]), str(example[\"a\"]), str(example[\"b\"]), str(example[\"c\"]), str(example[\"d\"]))\n",
        "    gold = str(example[\"answer\"]).strip().lower()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INSTRUCT}]},\n",
        "        {\"role\":\"user\",\"content\":[{\"type\":\"image\",\"image\":img},{\"type\":\"text\",\"text\":user_text}]},\n",
        "        {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":gold}]}\n",
        "    ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Qwen3 모델에 맞는 Chat Template 설정\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"qwen3-instruct\",\n",
        ")\n",
        "\n",
        "# 최종적으로 Trainer가 사용할 'text' 필드를 만드는 함수\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# Pandas DataFrame을 Hugging Face Dataset으로 변환\n",
        "raw_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# 데이터 포맷팅 적용\n",
        "formatted_dataset = raw_dataset.map(format_train_data, remove_columns=list(raw_dataset.features))\n",
        "final_dataset = formatted_dataset.map(formatting_prompts_func, batched=True, remove_columns=list(formatted_dataset.features))\n",
        "\n",
        "# 훈련/검증 데이터 분리\n",
        "dataset_split = final_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
        "train_ds = dataset_split[\"train\"]\n",
        "valid_ds = dataset_split[\"test\"]\n",
        "\n",
        "print(\"Final training sample:\\n\", train_ds[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6trj7t9U-hv"
      },
      "source": [
        "# 6. SFTTrainer를 사용한 파인튜닝\n",
        "\n",
        "Unsloth와 `trl` 라이브러리의 `SFTTrainer`를 사용하여 모델 파인튜닝을 진행합니다. 이 방식은 기존의 수동 학습 루프보다 훨씬 간결하고 효율적입니다.\n",
        "\n",
        "- 200개 샘플 학습 시 약 5~10분 소요됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft_trainer"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_training(model) # 학습 모드 활성화\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = valid_ds,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        per_device_eval_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # 데모용, 실제로는 num_train_epochs=1 등으로 설정\n",
        "        # num_train_epochs = 1,\n",
        "        learning_rate = 1e-4,\n",
        "        logging_steps = 1,\n",
        "        evaluation_strategy = \"steps\",\n",
        "        eval_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = SEED,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        max_length = MAX_SEQ_LENGTH,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZuYBgypVItX"
      },
      "source": [
        "# 7. 추론 및 제출 파일 생성\n",
        "\n",
        "학습된 LoRA 어댑터를 사용하여 테스트 데이터에 대한 추론을 수행하고, `submission.csv` 파일을 생성합니다.\n",
        "\n",
        "- 전체 테스트 데이터 추론 시 약 30분~1시간 소요됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87S01vC0dCmc"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model) # 추론 모드 활성화\n",
        "\n",
        "# 모델의 응답에서 정답(a, b, c, d)만 추출하는 함수\n",
        "def extract_choice(text: str) -> str:\n",
        "    text = text.strip().lower()\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "    if not lines:\n",
        "        return \"a\" # 응답이 없는 경우 'a'로 대체\n",
        "    last = lines[-1]\n",
        "    if last in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "        return last\n",
        "    tokens = last.split()\n",
        "    for tok in tokens:\n",
        "        if tok in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "            return tok\n",
        "    return \"a\" # 정답을 찾지 못한 경우 'a'로 대체\n",
        "\n",
        "# 추론 루프\n",
        "preds = []\n",
        "for i in tqdm(range(len(test_df))):\n",
        "    row = test_df.iloc[i]\n",
        "    img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
        "    user_text = build_mc_prompt(row[\"question\"], row[\"a\"], row[\"b\"], row[\"c\"], row[\"d\"])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INSTRUCT}]},\n",
        "        {\"role\":\"user\",\"content\":[{\"type\":\"image\",\"image\":img}, {\"type\":\"text\",\"text\":user_text}]}\n",
        "    ]\n",
        "\n",
        "    # Chat Template 적용\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # 토큰화 및 GPU로 이동\n",
        "    inputs = tokenizer(text=[text], images=[img], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
        "\n",
        "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    preds.append(extract_choice(decoded_output))\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission = pd.DataFrame({\"id\": test_df[\"id\"], \"answer\": preds})\n",
        "submission.to_csv(\"/content/submission.csv\", index=False)\n",
        "print(\"Saved /content/submission.csv\")\n",
        "print(\"Submission file sample:\")\n",
        "print(submission.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}