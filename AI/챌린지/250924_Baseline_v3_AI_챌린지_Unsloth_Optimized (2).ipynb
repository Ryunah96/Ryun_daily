{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY2ef3IwSUbU"
      },
      "source": [
        "# Baseline v3 - Unsloth 최적화 버전\n",
        "\n",
        "이 베이스라인 코드는 기존 v3 코드를 **Unsloth 라이브러리**를 사용하여 최적화한 버전입니다.\n",
        "\n",
        "**주요 변경 사항:**\n",
        "- **Unsloth 적용**: 더 적은 메모리로 2배 이상 빠른 파인튜닝을 수행합니다.\n",
        "- **SFTTrainer 사용**: Transformers의 복잡한 수동 학습 루프를 `trl`의 `SFTTrainer`로 대체하여 코드를 간소화했습니다.\n",
        "- **`FastVisionModel` 사용**: Unsloth의 Vision-Language 모델 로딩 클래스를 사용하여 모델 로딩 및 양자화 과정을 단순화했습니다.\n",
        "\n",
        "Colab의 GPU 환경(T4 GPU)에서 개발되었습니다.\n",
        "- **런타임 > 런타임 유형 변경 > T4 GPU**로 설정해주세요.\n",
        "\n",
        "> 참고 : https://docs.unsloth.ai/models/qwen3-vl-run-and-fine-tune  \n",
        "> 참고 : https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B)-Vision-GRPO.ipynb#scrollTo=6fUaoYJEKgpb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HBjYvBZSzqr"
      },
      "source": [
        "# 1. 환경 준비\n",
        "\n",
        "Unsloth 및 최신 라이브러리를 설치합니다.\n",
        "\n",
        "- 아래 셀 실행 후 **런타임 > 세션 다시 시작**을 반드시 진행해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unsloth_install"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0wuwJLPTJpQ"
      },
      "source": [
        "# 2. 데이터 준비\n",
        "\n",
        "구글 드라이브를 마운트하고 대회 데이터를 압축 해제합니다.   \n",
        "(*kaggle로 진행하는 교육생들은 `/kaggle/input/`에 있는 데이터셋으로 진행해주세요!)\n",
        "\n",
        "- `train.csv`, `train/`\n",
        "- `test.csv`, `test/`\n",
        "- `sample_submission.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXxjjvUpTLMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6269838-7fb8-45e5-a905-ef24dd3086ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 구글드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW4YNdl0Tn70"
      },
      "outputs": [],
      "source": [
        "# 데이터 압축 해제 (약 1분 소요)\n",
        "!unzip -q \"/content/drive/My Drive/251024/data.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WO2OCFiTqDv"
      },
      "source": [
        "# 3. 라이브러리, 데이터, 설정 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QXMeMgFT5MT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c70071a-a9fb-46c6-e91f-77d89dc5edd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from unsloth import FastVisionModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 이미지 로드 시 픽셀 제한 해제\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "# 디바이스 GPU 우선 사용 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 설정값 정의\n",
        "MODEL_ID = \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\" # Unsloth의 4bit 양자화 모델\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "MAX_NEW_TOKENS = 8\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 데이터셋 로드\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df  = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# 학습 시간을 줄이기 위해 200개 샘플만 사용 (실제 대회에서는 전체 데이터를 사용하세요)\n",
        "train_df = train_df.sample(n=200, random_state=SEED).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLdJb59EULQv"
      },
      "source": [
        "# 4. Unsloth 모델 및 LoRA 어댑터 로딩\n",
        "\n",
        "Unsloth의 `FastVisionModel`을 사용하여 4bit 양자화된 모델과 토크나이저를 간편하게 로드합니다. 이후 LoRA 어댑터를 추가하여 학습 준비를 마칩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbinSBK_Ubgo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "0548fe545cb349a0a0f65aa1d2aedafd",
            "ed2dcdc7a20542f78b2c002bcf3bcf15",
            "8f70671c6d994f41b8bbf89ce27666db",
            "74a9f5f6bd494dd2a97613d169b5cc17",
            "f1e27c0bf5dd4040afef42144b8be010",
            "f5b16f99b8084ecfb895a3e1bb173be1",
            "e29794e9c60b43c28a53c13c8329aa4b",
            "0f15b5c76f73453d86413b1b2bca8c2d",
            "943e2ee0b6c24bf7907b836718517097",
            "c5ebf2cd886040a38b49be4251a9dcf1",
            "8c8234b2643e4beb9c1758fec8abb0f0",
            "f943218a810c4607b98740e9fc10c64a",
            "82059373dcc94107897836d5d85ef60a",
            "460f19de8d634560bc4b2129ea84fe88",
            "6ce59062c45a45088fbc635076667471",
            "21d748971ba3404a96627f7963cd6178",
            "c7fc20253e084ed29d847e5bdac3a764",
            "600765c54273402f8edbf8b79a63d6ab",
            "b5d787a67d4842309645ac6b3e041cc5",
            "8b004f66f5e24e63b9d7a08040b1a592",
            "7338654ad52a4af88240934957b40298",
            "833050c0fa4149d5a92ead476f3f1849",
            "43ea31082c684159ad22f9e18873a2b0",
            "19dab8c49afe493e81d05ef0b12dffd2",
            "1f09faea83054060b8b7cc07005253c2",
            "317e2ac68968454a968de6b46bd29fac",
            "1ed0b3a1e9854a52b80d452f5a646ba8",
            "53dff8b0adc846bf95487c3af2eeba39",
            "c941723746f04279b14da6d2223e796a",
            "45903244e60d41268640825a7c238ae9",
            "a6293678e0704c61953ef22dbf054fcd",
            "6995c74f6e9746e882985be0b16391e7",
            "03642c1a67314e0eb9c10e1d34824d0c",
            "1e07dee7341542bdaabfaaa9a09ca15d",
            "73361a80f5fe48558a04e9d1b7b214fc",
            "f4b54cfd028b45908c1bfe3358d6d1dd",
            "4c160be58ab5419399f310230e849a70",
            "16f068a5ba3547ec9bb43c8f6b9a812d",
            "16f3a3bd1a1541c99533d27dc6110d0b",
            "d4d0417821bb4fffb8efce9f0b804077",
            "7e235c178e524fbbabdcbaaf3a1756d8",
            "2d83865ded6a4791b5f53ff88b5a04e5",
            "5c63c0a183544741bfa143e223e15048",
            "c863657c150c4d2892f66b259a41901e",
            "203f6ac6d5c54c35956fb002a6e9bc86",
            "f6c160946bbe44e084165fae9c3f0db3",
            "4ec593a76b384875a7ee43fee4411ccb",
            "f6c7fbed34c140bda9524df53b98efb5",
            "a1156e936d074affbdd48af54eb3331d",
            "9991b8dca1db4946ac3549ae41baff41",
            "d7bee5aa194741fd83adf979546a9f44",
            "d5824d58c9c346189f076f8fc26e18ee",
            "acd042e37a244e7a809697933c8daf34",
            "f2fb8020792445e2a55f0b445ff5d1dc",
            "5b93883edfe44e5ab73fa9990fd2fe91",
            "a7230ef252784c0daef72fbe1c6e6244",
            "cee9fc4a7f9e49dd9ca8e9fc612e41c8",
            "966a5726faec4104ad4688498585c75d",
            "338486b4a9734a11b1858b884fe72ab9",
            "b1c5924e7f0440e19c7b94c3af7a8359",
            "346b8ac65078415dacb94d36012192a4",
            "2ab1b7c3000d40dba24c7c6e024ef87c",
            "cce1beefd8e949958a29e590fb2890e1",
            "55a9e2c4c2ba4c1abf82cf94b230c7a2",
            "b72b5b8344554af2b12a90fe8184596a",
            "8821e7916c9046e2a7abb3a14db668b8",
            "f47b6f853534451baa921404edd811f0",
            "000ed5fabc9740bfaf9808c56c20c3fe",
            "9aa716288ace42139102aab8e7856ee1",
            "e338cbfd69964f4a9a3de47e93c47d06",
            "4ad06f936ad4448da3dc95594a22c992",
            "6d59d2f2568d4f54bf568b0864a28660",
            "b0ca89fd65dd415991069811060f179d",
            "b8626df4e4cf49ccb2e09bd05184db3a",
            "a791005894f845ea9eea1774f7d1c6cb",
            "027df8ad9c834d1b8985f64be393ecce",
            "ac9710fd5f364a6993a59b465c64e661",
            "2486d06098684cc7a76d6d3df2e8989d",
            "3655857db06541bba5e438569e8bba50",
            "d272554d17fd4ebb9f7278d63c545d61",
            "eada69af8d9c486e94676d71386323b6",
            "5f4f3cecd4ae49488ec8ff855e3f0a52",
            "329f985260e442b9b48bd2fbb7536492",
            "1289b0fb550548109697d64a64f81cbb",
            "d395ac29a1ff4b42a3d2699745277c8f",
            "6f46d9c1c4c9400cb1d53179e109b566",
            "fbbde7357c5d4538933e1421599ca450",
            "81c27cbe2c854f5a87e141d9e2e63844",
            "9087cd71fef64b5898f6a01c8ef2e72b",
            "a2f6f2a9ef3b4b969208f77e007ed07e",
            "3323b863ba0f4801b8a99a03d194412c",
            "ce2def20b4154f31bc5fcd09a82b9b66",
            "8d14e88bd5ab452985adadd5542522fb",
            "2561e74c943c47e8a51eebc7e058e3b0",
            "3383ebf503134210a1f45d9c9395dc2f",
            "41dd2aade0984bf9a6b4f59e66f3181b",
            "61f99b6ee04f4aefbaca1e9805dc819a",
            "0140c5fd6ac24823bd5e233ae3966679",
            "5807fcb091ab4028b6053f07fa9ad82f",
            "1f310023d0344848956210c2d87366ba",
            "45458b0caafa4d448f8866947b6301e1",
            "9774a238f0554251bf737fc90741da0b",
            "e4dc3d6289a74914842a2b1ddfd63f30",
            "10ee923063c547c389569d40d2b9fe40",
            "aa44a224226c4711b1de774bda4d802c",
            "be9cc8d38fa24779829b0bbf0cd6bfaf",
            "4cb5059a29ee4ae6871ff52b9c19c25c",
            "68c8376ab3244a29a4d276f40459864c",
            "306446510e98429b8539fe239b9fb28d",
            "4a871aaf4bda4b86b673d4cf971d564c",
            "e76e97159d1949089f2ed9be0b9ab61e",
            "fe2ae8d0dfd24363847f4511eeb45e17",
            "111b52552f5d4d158dd0713547528fed",
            "eb590b630f834c4ca6f1c4313462d1f9",
            "8961b880a04546e2a06152cd81577b3b",
            "e15b0e20e4ff441f88997d5888583040",
            "1c18b044128642869cea76bce78364fd",
            "9e178a8b07d54b7f83fcf4b6c0d2a4c6",
            "d9ff3936ec7b4ea7940a16b2cd3f05c9",
            "f77c62d2fa3a45cab084703f8aad6b9c",
            "eb015ba6c5ad4a55bd6f21bb1722d691",
            "dfaab4aaa6bd445faac0f079edd0bafe",
            "73e9998438fe47cc8b72910e9fe29d94",
            "fe81704accd241e88ce396cf3edbbdba",
            "c216e0458fbc4bf8b718c316ac6c9993",
            "d84c0898918e4755956fb017ee8ac334",
            "540b70d9817c4d418635df0d6c8ede62",
            "9aeb4bda07074a5aa931c5d5443c3e0f",
            "3a1cdd3d9fae43418a4a5643cd8be59d",
            "de479e0b80894104aada5bff16de0812",
            "feda47ad2318481e8ac40ee7c3ccc281",
            "76cc216c24bd4f5bb1b83f5f47c18bc3",
            "048529c759dc489fbefbccb0d2923001",
            "5fe0bf17095248c9b1d344ea9c7078ab",
            "e449225b3d85476ab90ebcb1badd62a7",
            "44f20d5fc31c4d2b9e404b9bc3493218",
            "6a7000cbfd174ee4a389822b221f009a",
            "f369cd6122074139a96162cbfd17cbe1",
            "e5a9f4562757480b8c3d8066cebef46c",
            "36dddd9f372d426aad65918299222965",
            "00ce01ec54a142a09a42bed0a19e9237",
            "7c572d34279849af95ab794fbc3672c5",
            "9a8f594941354a6dbd3192f9233e9518",
            "5346b707e281431aa64facf8bc74d5b8",
            "134c641467a84f3ba3cd90a14b767186",
            "fe6a1bfeaaf74b51a7c60944a3e367b7",
            "c0abfe96ca4b445593550b7012da46cd",
            "506bdef3dbb141bc8801178e47d8ce1d",
            "71c688d0d4f24a948b4a65aa269a5637",
            "540a9c0947814f5fa7b561b89625ec56",
            "81178d2491d84d3081dddfdad5f9efab",
            "9562e6b426054cc4a110c5eafe1f3a17",
            "957a0ffd63a64566bae96bb315941db4",
            "22078b4d06f544aeb82835907062dc24",
            "7d24905cc8024f27a8942d87ce1fde8f",
            "50d86393bc644f1f8e1eab67b89c294a",
            "8108b7d540354e8faf695046cc93f666",
            "f91bb1c52a244da89daeec8530b80b27",
            "2aa498bb495e46ff9161ed4f6eae3b8d",
            "c1ee4ef1f075495793e792bbab735dde",
            "f99fff8f8fe243bb91057410f99f59d5",
            "977b4a2f22c44e5c9aca4c8daca8b34e",
            "fa9c4cf96a754325bf0e2410b57cd706",
            "1b47303ded6b4516b2083be8001de477",
            "bc92bcb9aa114241a5ca5ed34bdc45ee"
          ]
        },
        "outputId": "b34911cd-18cd-49da-98d2-180c994de45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.8: Fast Qwen3_Vl patching. Transformers: 4.57.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0548fe545cb349a0a0f65aa1d2aedafd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f943218a810c4607b98740e9fc10c64a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.72G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43ea31082c684159ad22f9e18873a2b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e07dee7341542bdaabfaaa9a09ca15d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "203f6ac6d5c54c35956fb002a6e9bc86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7230ef252784c0daef72fbe1c6e6244"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f47b6f853534451baa921404edd811f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2486d06098684cc7a76d6d3df2e8989d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9087cd71fef64b5898f6a01c8ef2e72b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f310023d0344848956210c2d87366ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e76e97159d1949089f2ed9be0b9ab61e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfaab4aaa6bd445faac0f079edd0bafe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "048529c759dc489fbefbccb0d2923001"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5346b707e281431aa64facf8bc74d5b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "video_preprocessor_config.json:   0%|          | 0.00/817 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d24905cc8024f27a8942d87ce1fde8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 51,346,944 || all params: 8,818,470,640 || trainable%: 0.5823\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = False, # Enable vLLM fast inference\n",
        "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "# LoRA 어댑터 추가\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,  # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True,  # False if not finetuning language layers\n",
        "    finetune_attention_modules = True,  # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True,  # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = SEED,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer.sft_trainer import DataCollatorForVisionLanguageModeling\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "collator = DataCollatorForVisionLanguageModeling(processor)"
      ],
      "metadata": {
        "id": "HAhQBM7jOKid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvJAhuBvUnRe"
      },
      "source": [
        "# 5. 프롬프트 템플릿 및 데이터 포맷팅\n",
        "\n",
        "학습 및 추론에 사용할 프롬프트 템플릿을 정의하고, Hugging Face `datasets` 라이브러리를 사용하여 데이터를 모델의 Chat Template에 맞게 변환합니다. 이 과정은 기존의 Custom Dataset/Dataloader를 대체합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas DataFrame을 Hugging Face Dataset으로 변환\n",
        "raw_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# Resize to (512, 512) and handle image loading errors\n",
        "def convert_to_rgb(example):\n",
        "    try:\n",
        "        example[\"decoded_image\"] = Image.open(example[\"path\"]).resize((512, 512)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {example['path']}: {e}\")\n",
        "        # Create a white dummy image\n",
        "        example[\"decoded_image\"] = Image.new(\"RGB\", (512, 512), color = 'white')\n",
        "    return example\n",
        "\n",
        "raw_dataset = raw_dataset.map(convert_to_rgb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "81b855150ca44925b14037c1ff9248f7",
            "66c829f0d88b429fa627ba1ffac204b9",
            "6c7e426a51c14e8fad83e85815a594ca",
            "a28ae462a3cc4a5aa331baa6bb7e36f2",
            "5282eb6976974c8fb5c6b37db0ac24aa",
            "7ebd33584d264a1299025090efb9f476",
            "f4567dc8ba86424f8ae3de75db2cfc9b",
            "1d7aae33578b40dc9fe783760556aa72",
            "2958d918c133494787cac94a1be6eb56",
            "2aa5ee4414f845bb847eeaf75b5305f1",
            "615c843e91bc41f9a78af1c6bc945c0f"
          ]
        },
        "id": "-zgbTBjw9QKA",
        "outputId": "98feea30-940b-40a7-9fce-ece34f372a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b855150ca44925b14037c1ff9248f7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_formatting",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "5ac3c8bbdcc44bb88b515f8f20f51748",
            "43ea56863079462da94566959e04ff6a",
            "7bd7f7c0c35c444ead8a330765e33b74",
            "f551c87ce2d049d7985d6e27d38db297",
            "a5d54cb63dfe4083b03d8d73f3e11312",
            "7a0314029827463189ba11039bfbe347",
            "092479ccc1084754920fe2c06a22180e",
            "52a0faa758754fdb9a0d1d2500f28d58",
            "4ad95a4326184750945f0b3ae525bba9",
            "058f27814c124d648ccb98fc1bf55f4a",
            "d3b8f9b790744cad9b5779a6d722de2b"
          ]
        },
        "outputId": "046553f3-2f93-40d9-ecaf-96767ba2c4e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ac3c8bbdcc44bb88b515f8f20f51748"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training sample:\n",
            " {'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': '사진 속 새는 어떤 종류의 새일까요?\\n(a) 앵무새\\n(b) 까마귀\\n(c) 비둘기\\n(d) 참새\\n\\n정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'a', 'type': 'text'}], 'role': 'assistant'}], 'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x7AA3526881D0>]}\n"
          ]
        }
      ],
      "source": [
        "# 시스템 프롬포트도 커스터마이징 필수! (정확률 5~15% 향상)\n",
        "SYSTEM_INSTRUCT = (\n",
        "    \"You are a helpful visual question answering assistant. \"\n",
        "    \"Answer using exactly one letter among a, b, c, or d. No explanation.\"\n",
        ")\n",
        "\n",
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    return (\n",
        "        f\"{question}\\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        \"정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.\"\n",
        "    )\n",
        "\n",
        "# 학습 데이터를 모델의 대화 형식(messages)으로 변환하는 함수\n",
        "def make_conversation(example):\n",
        "    user_text = build_mc_prompt(str(example[\"question\"]), str(example[\"a\"]), str(example[\"b\"]), str(example[\"c\"]), str(example[\"d\"]))\n",
        "    gold = str(example[\"answer\"]).strip().lower()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INSTRUCT}]},\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\":[\n",
        "                {\"type\":\"image\"}, # Placeholder for the image\n",
        "                {\"type\":\"text\",\"text\":user_text}  # The text part of the prompt\n",
        "                ]\n",
        "         },\n",
        "          {\n",
        "              \"role\":\"assistant\",\n",
        "              \"content\":[\n",
        "                  {\"type\":\"text\",\"text\":gold}\n",
        "                  ]\n",
        "            }\n",
        "    ]\n",
        "    # The actual image data is kept separate for the processor\n",
        "    return {\"messages\": messages, \"images\": [example[\"decoded_image\"]]}\n",
        "\n",
        "dataset = raw_dataset.map(make_conversation, remove_columns=[\"path\", \"a\", \"b\", \"c\", \"d\", \"question\", \"id\", \"answer\", \"decoded_image\"])\n",
        "\n",
        "formatted_dataset = dataset\n",
        "\n",
        "# 훈련/검증 데이터 분리\n",
        "dataset_split = formatted_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "valid_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(\"Final training sample:\\n\", train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = train_dataset[100][\"images\"]\n",
        "text = train_dataset[100][\"messages\"]\n",
        "\n",
        "print(image)\n",
        "print(\"=================================================================\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UVTm5m_IJ2w",
        "outputId": "be92a931-719f-427a-fae0-22333eda28bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x7AA35226D820>]\n",
            "=================================================================\n",
            "[{'content': [{'text': None, 'type': 'image'}, {'text': '이 사진에서 볼 수 있는 것은 무엇인가요?\\n(a) 공항과 비행기\\n(b) 도시의 전경과 시장\\n(c) 해변과 바다\\n(d) 산과 숲\\n\\n정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'b', 'type': 'text'}], 'role': 'assistant'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6trj7t9U-hv"
      },
      "source": [
        "# 6. SFTTrainer를 사용한 파인튜닝\n",
        "\n",
        "Unsloth와 `trl` 라이브러리의 `SFTTrainer`를 사용하여 모델 파인튜닝을 진행합니다. 이 방식은 기존의 수동 학습 루프보다 훨씬 간결하고 효율적입니다.\n",
        "\n",
        "공식 docs : https://huggingface.co/docs/trl/main/sft_trainer\n",
        "\n",
        "- 200개 샘플 학습 시 약 5~10분 소요됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft_trainer",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "1911c777-2d98-479e-a2b6-d93ceb5f7769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 180 | Num Epochs = 2 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 51,346,944 of 8,818,470,640 (0.58% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 06:56, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.778100</td>\n",
              "      <td>14.385533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.309900</td>\n",
              "      <td>8.594125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.770700</td>\n",
              "      <td>7.129172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.634100</td>\n",
              "      <td>6.449550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.574000</td>\n",
              "      <td>6.214262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.540600</td>\n",
              "      <td>6.159873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Qwen3VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "# --- 3. 모델 학습 ---\n",
        "\n",
        "# 모델을 학습 모드로 활성화 (기존과 동일)\n",
        "FastVisionModel.for_training(model)\n",
        "\n",
        "# SFTTrainer 설정 (기존과 동일)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    # packing=True,           # 여러 짧은 시퀀스를 하나로 묶어 학습 효율을 높입니다.\n",
        "    data_collator=collator,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=1e-4,\n",
        "        logging_steps=1,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=SEED,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False,\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 학습 시작\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZuYBgypVItX"
      },
      "source": [
        "# 7. 추론 및 제출 파일 생성\n",
        "\n",
        "학습된 LoRA 어댑터를 사용하여 테스트 데이터에 대한 추론을 수행하고, `submission.csv` 파일을 생성합니다.\n",
        "\n",
        "- 전체 테스트 데이터 추론 시 약 30분~1시간 소요됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87S01vC0dCmc"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model) # 추론 모드 활성화\n",
        "\n",
        "# 모델의 응답에서 정답(a, b, c, d)만 추출하는 함수\n",
        "def extract_choice(text: str) -> str:\n",
        "    text = text.strip().lower()\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "    if not lines:\n",
        "        return \"a\" # 응답이 없는 경우 'a'로 대체\n",
        "    last = lines[-1]\n",
        "    if last in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "        return last\n",
        "    tokens = last.split()\n",
        "    for tok in tokens:\n",
        "        if tok in [\"a\", \"b\", \"c\", \"d\"]:\n",
        "            return tok\n",
        "    return \"a\" # 정답을 찾지 못한 경우 'a'로 대체\n",
        "\n",
        "# 추론 루프\n",
        "preds = []\n",
        "for i in tqdm(range(len(test_df))):\n",
        "    row = test_df.iloc[i]\n",
        "    image = Image.open(row[\"path\"]).convert(\"RGB\")\n",
        "    user_text = build_mc_prompt(row[\"question\"], row[\"a\"], row[\"b\"], row[\"c\"], row[\"d\"])\n",
        "\n",
        "    # Updated prompt structure to correctly pass text\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INSTRUCT}]},\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\":[\n",
        "                {\"type\":\"image\"}, # Placeholder for the image\n",
        "                {\"type\":\"text\",\"text\":user_text}  # The text part of the prompt\n",
        "                ]\n",
        "         }\n",
        "      ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize = False,\n",
        "            add_generation_prompt = True, # Must add assistant\n",
        "    )\n",
        "\n",
        "    # Chat Template 적용 - Pass the list of dictionaries directly to the tokenizer\n",
        "    # Corrected tokenizer call to pass the text and image separately\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        prompt,\n",
        "        add_special_tokens = False,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        # use_cache=True\n",
        "    )\n",
        "\n",
        "    # skip_special_tokens=True로 특수 토큰 제거\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    preds.append({\"id\": row[\"id\"], \"answer\": extract_choice(text)})\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission = pd.DataFrame(preds, columns=['id', 'answer'])\n",
        "submission.to_csv(\"/content/submission.csv\", index=False)\n",
        "print(\"Saved /content/submission.csv\")\n",
        "print(\"Submission file sample:\")\n",
        "print(submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 나의 구글 드라이브 본래 작업 폴더에 저장\n",
        "drive_path = \"/content/drive/My Drive/251024/submission.csv\"\n",
        "df.to_csv(drive_path, index=False)\n",
        "print(f\"Saved to Google Drive: {drive_path}\")"
      ],
      "metadata": {
        "id": "FRM2OE5Kb78P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lora 어뎁터 저장\n",
        "# model.save_pretrained(\"ai_challenge_lora\")  # Local saving\n",
        "# tokenizer.save_pretrained(\"ai_challenge_lora\")"
      ],
      "metadata": {
        "id": "hl_d3E10k8u5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}